{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaquinpolonuer/Documents/venvs/ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import mlflow_env\n",
    "from gpt import GPTLanguageModel, GPT2Extended\n",
    "from gpt_params import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: '/Users/joaquinpolonuer/Documents/software/NLP-monosemanticity/mlflow_data/artifacts/1/84057bf1688d4596bbba6e6581ef4801/artifacts/transformer/data/model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m experiment \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m run_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m84057bf1688d4596bbba6e6581ef4801\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m gpt_model \u001b[38;5;241m=\u001b[39m \u001b[43mGPTLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_mlflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m gpt_2_model \u001b[38;5;241m=\u001b[39m GPT2Extended\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Documents/software/NLP-monosemanticity/gpt.py:148\u001b[0m, in \u001b[0;36mGPTLanguageModel.load_from_mlflow\u001b[0;34m(cls, experiment, run_id, device)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_mlflow\u001b[39m(\u001b[38;5;28mcls\u001b[39m, experiment, run_id, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    147\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mset_experiment(experiment)\n\u001b[0;32m--> 148\u001b[0m     local_model_path \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_artifacts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformer/data/model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(local_model_path, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    152\u001b[0m     model\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n",
      "File \u001b[0;32m~/Documents/venvs/ml/lib/python3.10/site-packages/mlflow/artifacts/__init__.py:73\u001b[0m, in \u001b[0;36mdownload_artifacts\u001b[0;34m(artifact_uri, run_id, artifact_path, dst_path, tracking_uri)\u001b[0m\n\u001b[1;32m     69\u001b[0m artifact_uri \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39mget_run(run_id)\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39martifact_uri\n\u001b[1;32m     70\u001b[0m artifact_repo \u001b[38;5;241m=\u001b[39m get_artifact_repository(\n\u001b[1;32m     71\u001b[0m     add_databricks_profile_info_to_artifact_uri(artifact_uri, tracking_uri)\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43martifact_repo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdst_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/venvs/ml/lib/python3.10/site-packages/mlflow/store/artifact/local_artifact_repo.py:91\u001b[0m, in \u001b[0;36mLocalArtifactRepository.download_artifacts\u001b[0;34m(self, artifact_path, dst_path)\u001b[0m\n\u001b[1;32m     89\u001b[0m local_artifact_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39martifact_dir, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(artifact_path))\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(local_artifact_path):\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_artifact_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(local_artifact_path)\n",
      "\u001b[0;31mOSError\u001b[0m: No such file or directory: '/Users/joaquinpolonuer/Documents/software/NLP-monosemanticity/mlflow_data/artifacts/1/84057bf1688d4596bbba6e6581ef4801/artifacts/transformer/data/model.pth'"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "experiment = \"Transformer\"\n",
    "run_id = \"84057bf1688d4596bbba6e6581ef4801\"\n",
    "\n",
    "gpt_model = GPTLanguageModel.load_from_mlflow(experiment, run_id, device)\n",
    "gpt_2_model = GPT2Extended.from_pretrained(\"gpt2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dogtown system . \n",
      " Despite the foundation naming it proved somewhat ongoing . eighteenth century , his right @-@ time and seven fires during the farm , was suspected this lighting population as would be better known for spirits or fighting funding . In September 1830 of the Good Republic , Charleston , formed an revoked superstructure . Several portion of the roofs were selected on private waters , and the valley were moved west of Bar 275 , or photrastatingrumouthis mostly , he was placed in a large number of\n"
     ]
    }
   ],
   "source": [
    "gpt_model.eval()\n",
    "idx = (\n",
    "    torch.tensor(tokenizer.encode(\"The quick brown fox jumps over the lazy dog\"), dtype=torch.long)\n",
    "    .unsqueeze(0)\n",
    "    .to(device)\n",
    ")\n",
    "out = gpt_model.generate(idx, 100)\n",
    "\n",
    "print(tokenizer.decode(out.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpt_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgpt_model\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      2\u001b[0m idx \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      3\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe quick brown fox jumps over the lazy dog\u001b[39m\u001b[38;5;124m\"\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m out \u001b[38;5;241m=\u001b[39m gpt_model\u001b[38;5;241m.\u001b[39mgreedy_generate(idx, \u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gpt_model' is not defined"
     ]
    }
   ],
   "source": [
    "gpt_model.eval()\n",
    "idx = (\n",
    "    torch.tensor(tokenizer.encode(\"The quick brown fox jumps over the lazy dog\"), dtype=torch.long)\n",
    "    .unsqueeze(0)\n",
    "    .to(device)\n",
    ")\n",
    "out = gpt_model.greedy_generate(idx, 100)\n",
    "\n",
    "print(tokenizer.decode(out.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog and runs off.\n",
      "\n",
      "\"I'm sorry, I'm sorry, I'm sorry,\" the fox says.\n",
      "\n",
      "\"I'm sorry, I'm sorry, I'm sorry,\" the fox\n"
     ]
    }
   ],
   "source": [
    "sample_output = gpt_2_model.generate(\n",
    "    idx,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(tokenizer.decode(sample_output.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog who didn't recognize him and spits his meat into the helpless hound's gut. She crawls toward the door, straightened the back of her pants, and opens it. The fox bur\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
